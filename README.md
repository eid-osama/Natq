<p align="center">
  <img src="Media\images\Fully_Logo_White.png" alt="Natq Logo" width="800"/>
</p>

<h1 align="center">ğŸ—£ï¸ Natq â€“ Arabic Text-to-Speech (TTS) System</h1>

<p align="center">

## ğŸ“Œ Overview

Natq is an open-source Arabic Text-to-Speech (TTS) system that uses deep learning models to convert Arabic text into natural-sounding speech.  
We designed and trained multiple architectures on public datasets with a focus on Arabic language intricacies, including diacritization and phoneme modeling.

---
<p align="center">
<img src="Media\images\Helwan_University_Logo.jpg" alt="Natq Logo" width="300"/>
</p>
<p align="center">
<b>Graduation Project â€“ Faculty of Computing and Artificial Intelligence, Helwan University<br>
Department of Artificial Intelligence<br>
Supervised by: Dr. Yasser Hifny & Dr. Ahmed Hesham
</b>
  </p>

## ğŸ¯ Objectives

- Create a modular and efficient Arabic TTS system using public datasets.
- Handle Arabic-specific challenges such as diacritics and phoneme mapping.
- Compare performance of different spectrogram generators and vocoders.
- Support live speech synthesis via a web app (React + FastAPI).

---

## ğŸ§  Architecture

The system pipeline is composed of:

1. **Text Preprocessing**
2. **Diacritization**: [CATT Transformer](https://arxiv.org/abs/2306.07076)
3. **Grapheme-to-Phoneme (G2P)**: Nawar Halabiâ€™s Phenomizer
4. **Spectrogram Generation**: FastPitch, FastSpeech2, Mixer-TTS, or Spark-TTS
5. **Vocoder**: HiFi-GAN for waveform synthesis

> ![Pipeline Diagram](images/tts_pipeline.png)

---

## ğŸ“š Datasets

| Dataset   | Hours | Diacritized | Accent        | Notes                       |
|-----------|-------|-------------|---------------|-----------------------------|
| Arabic_Speech_Corpus    | 4.1   | âœ…          | Levantine     | Open-source                 |
| ClArTTS   | 12    | âœ…          | Classical MSA | Based on LibriVox audiobook |

---

## ğŸ›  Models Used

### ğŸ”¹ Spectrogram Generators

- **FastPitch** â€“ Parallel and pitch-controllable TTS
- **FastSpeech2** â€“ Variance-aware and efficient
- **Mixer-TTS** â€“ MLP-Mixer based parallel synthesis
- **Spark-TTS** â€“ End-to-end LLM-based TTS with zero-shot speaker cloning

### ğŸ”¹ Vocoder

- **HiFi-GAN** â€“ Fast, high-fidelity waveform generation

> ![Spectrogram Comparison](images/spectrogram_comparison.png)

---

## ğŸ§ª Evaluation

### Objective

- Spectrogram quality
- Inference time
- Model size

### Subjective

- Mean Opinion Score (MOS) from human listeners

> ![MOS Table](images/mos_table.png)

---

<!-- ## ğŸ”ˆ Audio Samples

| Model      | Dataset  | Sample             |
|------------|----------|--------------------|
| FastPitch  | ClArTTS  | ğŸ”Š [Listen](audio_samples/fastpitch_sample.wav)  
| Mixer-TTS  | ASC      | ğŸ”Š [Listen](audio_samples/mixer_tts_sample.wav)  
| Spark-TTS  | ClArTTS  | ğŸ”Š [Listen](audio_samples/spark_tts_sample.wav)  
| HiFi-GAN   | â€”        | ğŸ”Š [Listen](audio_samples/hifigan_sample.wav) -->

## ğŸ”ˆ Audio Samples

<h1 align="center">ÙˆÙØ§Ù„Ø³ÙÙ‘Ù„ÙØ§Ù…Ù Ø¹ÙÙ„ÙÙ‰ Ø£ÙØ´Ù’Ø±ÙÙÙ Ø§Ù„Ù’Ø£ÙÙ†Ù’Ø¨ÙÙŠÙØ§Ø¡Ù ÙˆÙØ§Ù„Ù’Ù…ÙØ±Ù’Ø³ÙÙ„ÙÙŠÙ†Ù Ø³ÙÙŠÙÙ‘Ø¯ÙÙ†ÙØ§ Ù…ÙØ­ÙÙ…ÙÙ‘Ø¯Ù</h1>
  <table>
    <thead>
      <tr>
        <th>Model</th>
        <th>Dataset</th>
        <th>Sample</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>FastPitch</td>
        <td>ClArTTS</td>
        <td>
          <audio controls>
            <source src="Media/audio_samples/FastPitch-TTS_MOS/walsalam_fp.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </td>
      </tr>
      <tr>
        <td>Mixer-TTS</td>
        <td>ASC</td>
        <td>
          <audio controls>
            <source src="Media/audio_samples/Mixer-TTS_MOS/walsalam_mx.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </td>
      </tr>
      <tr>
        <td>Spark-TTS</td>
        <td>ClArTTS</td>
        <td>
          <audio controls>
            <source src="Media/audio_samples/Spark-TTS_MOS/salam_spark.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </td>
      </tr>
      <tr>
        <td>VITS_facebook</td>
        <td>â€”</td>
        <td>
          <audio controls>
            <source src="Media/audio_samples/speech_VITS_facebook_output.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </td>
      </tr>
      <tr>
        <td>T5_MBZUAI</td>
        <td>â€”</td>
        <td>
          <audio controls>
            <source src="Media/audio_samples/speech_T5_MBZUAI_output.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </td>
      </tr>
      <tr>
        <td>FastSpeech2</td>
        <td>â€”</td>
        <td>
          <audio controls>
            <source src="audio_samples/hifigan_sample.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </td>
      </tr>
    </tbody>
  </table>

---

## ğŸ‘¥ Contributors

- [Abdelhalim Ashraf](https://github.com/ABDELHALIM9)
- [Abdelrahman Ramadan](https://github.com/Abdelrhman-Ramadan)
- [Eid Osama Eid](https://github.com/eid-osama)
- [Omar Tamer](https://github.com/Omartamer783)
- [Ali Adel](http://github.com/ali-adel)

---

## ğŸ“‚ Project Structure

```graphql
d:/coding/Natq/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ FastPitch/
â”‚   â”œâ”€â”€ .vscode/
â”‚   â”‚   â””â”€â”€ sttings.json
â”‚   â”œâ”€â”€ arabic_phoneme_tokenizer.py
â”‚   â”œâ”€â”€ catt/
â”‚   â”œâ”€â”€ custom_arabic_to_phones.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ FastSpeech2/
â”‚   â”œâ”€â”€ arabic_phoneme_tokenizer.py
â”‚   â”œâ”€â”€ catt/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Media/
â”‚   â”œâ”€â”€ audio_samples/
â”‚   â”‚   â”œâ”€â”€ FastPitch-TTS_MOS/
â”‚   â”‚   â”‚   â”œâ”€â”€ bism_fp.mp4
â”‚   â”‚   â”‚   â”œâ”€â”€ bism_fp.wav
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ Mixer-TTS_MOS/
â”‚   â”‚   â”‚   â”œâ”€â”€ bism_mix.mp4
â”‚   â”‚   â”‚   â”œâ”€â”€ bism_mix.wav
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ Spark-TTS_MOS/
â”‚   â”‚       â”œâ”€â”€ bism_spark.mp4
â”‚   â”‚       â”œâ”€â”€ bism_spark.wav
â”‚   â”‚       â””â”€â”€ ...
â”‚   â””â”€â”€ images/
â”‚       â”œâ”€â”€ Demo.jpg
â”‚       â””â”€â”€ Fully_Logo_White.png
â”œâ”€â”€ Natq-Frontend/
â”‚   â”œâ”€â”€ .gitignore
â”‚   â”œâ”€â”€ eslint.config.js
â”‚   â””â”€â”€ ...
â”œâ”€â”€ README.md
â””â”€â”€ Spark/
    â”œâ”€â”€ catt/
    â”œâ”€â”€ download_model_files.ipynb
    â””â”€â”€ ...

```

---

## ğŸŒ Web Application

We built a simple TTS web interface using **FastAPI** for the backend and **React** for the frontend.

<p align="center">
  <img src="Media\images\Demo.jpg" alt="Natq Logo" width="800"/>
</p>

ğŸ§ª Test it locally:

```bash
cd backend
uvicorn main:app --reload
```

# Then in another terminal

```bash
cd ../app
npm install
npm start
```
